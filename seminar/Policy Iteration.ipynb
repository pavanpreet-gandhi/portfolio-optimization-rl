{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an implementation of policy iteration exactly as described in [Sutton and Barroâ€™s Introduction to Reinforcement Learning](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf). I have also used [Stanford Universities CS234](https://web.stanford.edu/class/cs234/assignments.html) assignment 1 as a template. Some other resources include [Deep Lizard's RL course](https://deeplizard.com/course/rlcpailzrd) and [Steve Brunton's YouTube playlist on RL](https://www.youtube.com/playlist?list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Game: [Frozen Lake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)**\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following:\n",
    "\n",
    "`SFFF`<br>\n",
    "`FHFH`<br>\n",
    "`FFFH`<br>\n",
    "`HFFG`\n",
    "\n",
    "`S`: starting point, safe  \n",
    "`F`: frozen surface, safe  \n",
    "`H`: hole, fall to your doom  \n",
    "`G`: goal, where the frisbee is located\n",
    "\n",
    "Your task is to use use *policy iteration* and *value iteration* to come up with an optimal policy for crossing the lake. Frame the task as an MDP with the following reward structure:\n",
    "- You receive a reward of 1 if you reach the goal, and 0 otherwise.\n",
    "- The episode ends when you reach the goal or fall in a hole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version: 0.21.0\n"
     ]
    }
   ],
   "source": [
    "print('Gym version:', gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
    "the parameters P, nS, nA, gamma are defined as follows:\n",
    "\n",
    "\tP: nested dictionary\n",
    "\t\tFrom gym.core.Environment\n",
    "\t\tFor each pair of states in [0, nS-1] and actions in [0, nA-1], P[state][action] is a\n",
    "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
    "\t\t\t- probability: float\n",
    "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "\t\t\t- nextstate: int\n",
    "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
    "\t\t\t- reward: int\n",
    "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
    "\t\t\t\t\"nextstate\" with \"action\"\n",
    "\t\t\t- terminal: bool\n",
    "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "\tnS: int\n",
    "\t\tnumber of states in the environment\n",
    "\tnA: int\n",
    "\t\tnumber of actions in the environment\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3, max_iter=100):\n",
    "    \"\"\"\n",
    "    Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "    \n",
    "    value_function = np.zeros(nS)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        delta = 0\n",
    "        \n",
    "        for s in range(nS):\n",
    "            v = value_function[s] # save a copy of original value of state s\n",
    "            a = policy[s] # action to take according to policy\n",
    "            value_function[s] = sum([prob*(reward + gamma*value_function[next_S]) for prob, next_S, reward, term in P[s][a]])\n",
    "            delta = max(delta, abs(v-value_function[s]))\n",
    "        \n",
    "        if delta<tol:\n",
    "            break\n",
    "        \n",
    "        if i==max_iter-1:\n",
    "            print(f'Policy evaluation failed to converge after {max_iter} iterations')\n",
    "    \n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test (should return reasonable value function, albeit random and usually all zero)\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "policy = np.random.randint(low=0, high=env.nA, size=env.nS)\n",
    "policy_evaluation(env.P, env.nS, env.nA, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "\t\"\"\"\n",
    "\tGiven the value function from policy improve the policy.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\tvalue_from_policy: np.ndarray\n",
    "\t\tThe value calculated from the policy\n",
    "\tpolicy: np.array\n",
    "\t\tThe previous policy.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tnew_policy: np.ndarray[nS]\n",
    "\t\tAn array of integers. Each integer is the optimal action to take\n",
    "\t\tin that state according to the environment dynamics and the\n",
    "\t\tgiven value function.\n",
    "\t\"\"\"\n",
    "\n",
    "\tnew_policy = np.zeros(nS, dtype='int')\n",
    "\n",
    "\tpolicy_stable = True\n",
    "\tfor s in range(nS):\n",
    "\t\taction_values = [sum([prob*(reward + gamma*value_from_policy[next_S]) for prob, next_S, reward, term in P[s][a]]) for a in range(nA)]\n",
    "\t\tnew_policy[s] = np.argmax(action_values)\n",
    "\t\tif new_policy[s]!=policy[s]:\n",
    "\t\t\tpolicy_stable = False\n",
    "\n",
    "\treturn new_policy, policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 2, 2, 3, 0, 0, 0, 3, 3, 1, 0, 0, 2, 1, 0]), False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test (should return reasonable policy, albeit random)\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
    "policy = np.random.randint(low=0, high=env.nA, size=env.nS)\n",
    "value_from_policy = np.random.random(size=env.nS)*10\n",
    "policy_improvement(env.P, env.nS, env.nA, value_from_policy, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3, max_iter=100):\n",
    "\t\"\"\"\n",
    "\tRuns policy iteration.\n",
    "\n",
    "\tYou should call the policy_evaluation() and policy_improvement() methods to\n",
    "\timplement this method.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\ttol parameter used in policy_evaluation()\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "\t\"\"\"\n",
    "\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    " \n",
    "\tfor i in range(max_iter):\n",
    "\n",
    "\t\tvalue_function = policy_evaluation(P, nS, nA, policy)\n",
    "\t\tpolicy, policy_stable = policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
    "\n",
    "\t\tif policy_stable:\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\tif i==max_iter-1:\n",
    "\t\t\tprint(f'Policy iteration failed to converge after {max_iter} iterations')\n",
    "\n",
    "\treturn value_function, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "\t\"\"\"\n",
    "\tLearn value function and policy by using value iteration method for a given\n",
    "\tgamma and environment.\n",
    "\n",
    "\tParameters:\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\tTerminate value iteration when\n",
    "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "\t\"\"\"\n",
    "\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\n",
    "\n",
    "\t############################\n",
    "\treturn value_function, policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: \n",
      " [[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]] \n",
      "\n",
      "Optimal value function: \n",
      " [[0.59049 0.6561  0.729   0.6561 ]\n",
      " [0.6561  0.      0.81    0.     ]\n",
      " [0.729   0.81    0.9     0.     ]\n",
      " [0.      0.9     1.      0.     ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False) # what happens if we change is_slippery to true?\n",
    "\n",
    "value_function, policy = policy_iteration(env.P, env.nS, env.nA) # optimal value function and policy\n",
    "\n",
    "print('Optimal policy: \\n', policy.reshape(4,4), '\\n')\n",
    "print('Optimal value function: \\n', value_function.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "        Environment to play on. Must have nS, nA, and P as\n",
    "        attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "        The action to take at a given state\n",
    "    \"\"\"\n",
    "    \n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "    for t in range(max_steps):\n",
    "        env.render()\n",
    "        sleep(0.2)\n",
    "        action = policy[state]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        clear_output(wait=True)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f'Episode reward: {episode_reward}')\n",
    "    else:\n",
    "        print(f'Agent did not reach a terminal state in {max_steps} steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "play_game(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try another game with a larger state space, [taxi](https://www.gymlibrary.dev/environments/toy_text/taxi/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: \n",
      " [4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 3\n",
      " 0 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 4 4 4 4 0 0 0 0 0 0 0 0 0 5 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 2 2 2\n",
      " 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 1\n",
      " 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 2 2 2 2 0 0 0 0 2 2 2 2 1 2 0 2 1 1\n",
      " 1 1 2 2 2 2 3 3 3 3 2 2 2 2 1 2 3 2 3 3 3 3 2 2 2 2 3 3 3 3 2 2 2 2 3 2 3\n",
      " 2 3 3 3 3 2 2 2 2 3 3 3 3 0 0 0 0 3 2 3 0 3 3 3 3 1 1 1 1 3 3 3 3 0 0 0 0\n",
      " 3 1 3 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 2 2 2 2 1 1 1 1 2\n",
      " 2 2 2 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1\n",
      " 1 1 0 0 0 0 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 4 4 4 4 1 1 1 1 1 1 5 1 1 1 1 1 2 2 2 2 1 1 1 1 2 2 2 2 1 2 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 1 1 1 1 4 4 4 4 1 2 1 5 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 1 1 1 3] \n",
      "\n",
      "Optimal value function: \n",
      " [ 89.47173769  32.81719251  55.26171741  37.57388511   8.43072799\n",
      "  32.81719251   8.42970771  15.28080619  32.81821279  18.09133972\n",
      "  55.26171741  21.21182645  12.75444177  18.09133972  12.75342148\n",
      "  37.57388511 100.52456392  37.57528467  62.51364567  42.8607315\n",
      "  79.52456392  28.53547326  48.73554567  32.8164966   10.47899759\n",
      "  37.57528467  10.47807934  18.09064381  28.53639151  15.28220575\n",
      "  48.73554567  18.09064381  15.28312401  21.21322602  15.28220575\n",
      "  42.8607315   89.47210753  42.8619911   55.2622811   48.73491935\n",
      "  42.86281753  12.75398518  24.68192593  15.28157943  24.68275236\n",
      "  70.5712811   24.68192593  37.57465835  24.68275236  12.75398518\n",
      "  42.8619911   15.28157943  18.09272984  24.68192593  18.09190342\n",
      "  48.73491935  48.73679678  79.52415299  48.73605299  55.26171741\n",
      "  37.57653578  10.47858666  21.21373334  12.75342148  28.53672437\n",
      "  79.52415299  28.53598058  42.86142741  21.21447713  10.47858666\n",
      "  37.57579199  12.75342148  21.21447713  28.53598058  21.21373334\n",
      "  55.26171741  42.8631171   89.47173769  42.86244769  62.51364567\n",
      "  32.8188822    8.43072799  18.09236001  10.47807934  32.8188822\n",
      "  89.47173769  32.81821279  48.73554567  18.09302941   8.43072799\n",
      "  32.81821279  10.47807934  18.09302941  24.68238252  18.09236001\n",
      "  48.73554567  37.57680539 100.52456392  37.57620292  55.2622811\n",
      "  79.52456392  28.53547326  48.73554567  32.8164966   10.47899759\n",
      "  37.57528467  10.47807934  18.09064381  37.57620292  21.21322602\n",
      "  62.51364567  24.68066633  15.28312401  21.21322602  15.28220575\n",
      "  42.8607315   89.47210753  42.8619911   70.5712811   48.73491935\n",
      "  70.57210753  24.68192593  42.8619911   28.53484694  12.75481161\n",
      "  42.8619911   12.75398518  21.21259969  32.81858263  18.09190342\n",
      "  55.2622811   21.21259969  18.09272984  24.68192593  18.09190342\n",
      "  48.73491935  79.52489678  48.73605299  62.51415299  55.26171741\n",
      "  48.73679678  15.28271307  28.53598058  18.09133972  21.21447713\n",
      "  62.51415299  21.21373334  32.81719251  28.53672437  15.28271307\n",
      "  48.73605299  18.09133972  21.21447713  28.53598058  21.21373334\n",
      "  55.26171741  55.2634071   70.57173769  55.26273769  62.51364567\n",
      "  42.8631171   12.75444177  24.68238252  15.28220575  24.68305193\n",
      "  70.57173769  24.68238252  37.57528467  24.68305193  12.75444177\n",
      "  42.86244769  15.28220575  24.68305193  32.81821279  24.68238252\n",
      "  62.51364567  48.73706639  79.52456392  48.73646392  70.5712811\n",
      "  37.57680539  10.47899759  21.21414427  12.75398518  28.53699398\n",
      "  79.52456392  28.53639151  42.8619911   21.21474674  10.47899759\n",
      "  37.57620292  12.75398518  21.21474674  28.53639151  21.21414427\n",
      "  55.2622811   42.86335975  89.47210753  42.86281753  62.51415299\n",
      "  70.57210753  24.68192593  42.8619911   28.53484694  12.75481161\n",
      "  42.8619911   12.75398518  21.21259969  42.86281753  24.68192593\n",
      "  70.5712811   28.53484694  18.09272984  24.68192593  18.09190342\n",
      "  48.73491935  79.52489678  48.73605299  79.52415299  55.26171741\n",
      "  62.51489678  21.21373334  37.57579199  24.68136224  15.28345686\n",
      "  48.73605299  15.28271307  24.68136224  37.57653578  21.21373334\n",
      "  62.51415299  24.68136224  21.21447713  28.53598058  21.21373334\n",
      "  55.26171741  70.5724071   55.26273769  70.57173769  62.51364567\n",
      "  55.2634071   18.09236001  32.81821279  21.21322602  18.09302941\n",
      "  55.26273769  18.09236001  28.53547326  32.8188822   18.09236001\n",
      "  55.26273769  21.21322602  24.68305193  32.81821279  24.68238252\n",
      "  62.51364567  62.51516639  62.51456392  62.51456392  70.5712811\n",
      "  48.73706639  15.28312401  28.53639151  18.09190342  21.21474674\n",
      "  62.51456392  21.21414427  32.8177562   28.53699398  15.28312401\n",
      "  48.73646392  18.09190342  28.53699398  37.57620292  28.53639151\n",
      "  70.5712811   55.26364975  70.57210753  55.26310753  79.52415299\n",
      "  42.86335975  12.75481161  24.68275236  15.28271307  24.68329458\n",
      "  70.57210753  24.68275236  37.57579199  24.68329458  12.75481161\n",
      "  42.86281753  15.28271307  24.68329458  32.81858263  24.68275236\n",
      "  62.51415299  48.73728478  79.52489678  48.73679678  70.57173769\n",
      "  62.51489678  21.21373334  37.57579199  24.68136224  10.47933044\n",
      "  37.57579199  10.47858666  18.09133972  48.73679678  28.53598058\n",
      "  79.52415299  32.81719251  15.28345686  21.21373334  15.28271307\n",
      "  42.86142741  70.5724071   42.86244769  89.47173769  48.73554567\n",
      "  55.2634071   18.09236001  32.81821279  21.21322602  12.75511117\n",
      "  42.86244769  12.75444177  21.21322602  32.8188822   18.09236001\n",
      "  55.26273769  21.21322602  18.09302941  24.68238252  18.09236001\n",
      "  48.73554567  62.51516639  48.73646392  62.51456392  55.2622811\n",
      "  48.73706639  15.28312401  28.53639151  18.09190342  15.28372647\n",
      "  48.73646392  15.28312401  24.68192593  28.53699398  15.28312401\n",
      "  48.73646392  18.09190342  21.21474674  28.53639151  21.21414427\n",
      "  55.2622811   55.26364975  55.26310753  55.26310753  62.51415299\n",
      "  42.86335975  12.75481161  24.68275236  15.28271307  18.09327206\n",
      "  55.26310753  18.09272984  28.53598058  24.68329458  12.75481161\n",
      "  42.86281753  15.28271307  32.81912485  42.86281753  32.81858263\n",
      "  79.52415299  48.73728478  62.51489678  48.73679678  89.47173769\n",
      "  37.57702378  10.47933044  21.21447713  12.75444177  21.21496512\n",
      "  62.51489678  21.21447713  32.81821279  21.21496512  10.47933044\n",
      "  37.57653578  12.75444177  28.53721237  37.57653578  28.53672437\n",
      "  70.57173769  42.8635563   70.5724071   42.8631171   79.52456392\n",
      "  55.2634071   18.09236001  32.81821279  21.21322602   8.4313974\n",
      "  32.81821279   8.43072799  15.28220575  55.2634071   32.81821279\n",
      "  89.47173769  37.57528467  12.75511117  18.09236001  12.75444177\n",
      "  37.57528467  62.51516639  37.57620292 100.52456392  42.8619911\n",
      "  48.73706639  15.28312401  28.53639151  18.09190342  10.47960006\n",
      "  37.57620292  10.47899759  18.09190342  28.53699398  15.28312401\n",
      "  48.73646392  18.09190342  15.28372647  21.21414427  15.28312401\n",
      "  42.8619911   55.26364975  42.86281753  55.26310753  48.73605299\n",
      "  42.86335975  12.75481161  24.68275236  15.28271307  12.75535383\n",
      "  42.86281753  12.75481161  21.21373334  24.68329458  12.75481161\n",
      "  42.86281753  15.28271307  18.09327206  24.68275236  18.09272984\n",
      "  48.73605299  48.73728478  48.73679678  48.73679678  55.26273769\n",
      "  37.57702378  10.47933044  21.21447713  12.75444177  15.28394486\n",
      "  48.73679678  15.28345686  24.68238252  21.21496512  10.47933044\n",
      "  37.57653578  12.75444177  37.57702378  48.73679678  37.57653578\n",
      "  89.47173769  42.8635563   55.2634071   42.8631171  100.52456392\n",
      "  32.8193214    8.4313974   18.09302941  10.47899759  18.09346861\n",
      "  55.2634071   18.09302941  28.53639151  18.09346861   8.4313974\n",
      "  32.8188822   10.47899759  32.8193214   42.8631171   32.8188822\n",
      "  79.52456392  37.57720067  62.51516639  37.57680539  89.47210753]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "value_function, policy = policy_iteration(env.P, env.nS, env.nA) # optimal value function and policy\n",
    "\n",
    "print('Optimal policy: \\n', policy, '\\n')\n",
    "print('Optimal value function: \\n', value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Episode reward: 9\n"
     ]
    }
   ],
   "source": [
    "play_game(env, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rl_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6869a3fcd5fc665c02cfd6671afb83b30427e0c0f4ed20c6135df23c280a7ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
