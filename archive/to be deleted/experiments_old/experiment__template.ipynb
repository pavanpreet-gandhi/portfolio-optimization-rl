{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the current working directory the project root directory\n",
    "import os\n",
    "PROJECT_ROOT_DIRECTORY = 'senior_project'\n",
    "while os.path.basename(os.getcwd()) != PROJECT_ROOT_DIRECTORY:\n",
    "    os.chdir(os.pardir)\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from stable_baselines3 import DQN\n",
    "from environments.discrete_env_v1 import PortfolioManagementEnv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data\\dow_10_returns_train.csv', index_col=0, parse_dates=True) # TODO\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS = ['AXP', 'AAPL', 'BA', 'GS', 'INTC', 'JNJ', 'KO', 'NKE', 'PG', 'DIS']\n",
    "\n",
    "RETURN_COLS = ['RF_RETURN'] + [f'{ticker}_RETURN' for ticker in TICKERS]\n",
    "\n",
    "FEATURE_COLS = RETURN_COLS # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = PortfolioManagementEnv(\n",
    "    train_df, \n",
    "    RETURN_COLS, \n",
    "    FEATURE_COLS, \n",
    "    window_size = 10, # TODO\n",
    "    episode_length = 90 # TODO\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files and Folders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(RUN ONCE)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_id = ''\n",
    "# experiment_id = f'EXPERIMENT_N_{int(time.time())}' # TODO\n",
    "print(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO\n",
    "# models_dir = f'experiments/models/{experiment_id}'\n",
    "# log_dir = f'experiments/logs'\n",
    "\n",
    "# if not os.path.exists(models_dir):\n",
    "#     os.makedirs(models_dir)\n",
    "\n",
    "# if not os.path.exists(log_dir):\n",
    "#     os.makedirs(log_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(RUN ONCE)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO\n",
    "# model = DQN(\n",
    "#     'MlpPolicy', \n",
    "#     train_env, \n",
    "#     verbose = 1, \n",
    "#     tensorboard_log = log_dir\n",
    "# )\n",
    "\n",
    "# TIMESTEPS = 10_000 # Number of timesteps between saves\n",
    "# for i in range(1,300):\n",
    "#     model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=experiment_id)\n",
    "#     model.save(f'{models_dir}/{TIMESTEPS*i}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 000_000 --> info\n",
    "# 000_000 --> info\n",
    "\n",
    "model_number = 100_000 # TODO\n",
    "model = DQN.load(f'experiments/models/{experiment_id}/{model_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_env(env, title=''):\n",
    "    obs, done = env.reset(), False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    plt.title(title)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_performance = PortfolioManagementEnv(\n",
    "    train_df, \n",
    "    RETURN_COLS, \n",
    "    FEATURE_COLS, \n",
    "    window_size = 10, # TODO\n",
    "    episode_length = -1\n",
    ")\n",
    "\n",
    "val_df = pd.read_csv('data\\dow_10_returns_val.csv', index_col=0, parse_dates=True)\n",
    "val_env = PortfolioManagementEnv(\n",
    "    val_df, \n",
    "    RETURN_COLS, \n",
    "    FEATURE_COLS, \n",
    "    window_size = 10, # TODO\n",
    "    episode_length = -1\n",
    ")\n",
    "\n",
    "evaluate_env(train_env_performance, 'Train Data')\n",
    "evaluate_env(val_env, 'Val Data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('experiments/results_df.csv', index_col=0, parse_dates=True)\n",
    "results_df[f'{experiment_id}_TRAIN'] = train_env_performance.get_portfolio_returns()\n",
    "results_df[f'{experiment_id}_VAL'] = val_env.get_portfolio_returns()\n",
    "\n",
    "(1+results_df[~results_df[f'{experiment_id}_TRAIN'].isna()]).cumprod().plot(title='Train Performance', figsize=(12,4));\n",
    "(1+results_df[~results_df[f'{experiment_id}_TRAIN'].isna()])[f'{experiment_id}_TRAIN'].cumprod().plot(color='black', grid=True);\n",
    "\n",
    "(1+results_df[~results_df[f'{experiment_id}_VAL'].isna()]).cumprod().plot(title='Validation Performance', figsize=(12,4));\n",
    "(1+results_df[~results_df[f'{experiment_id}_VAL'].isna()])[f'{experiment_id}_VAL'].cumprod().plot(color='black', grid=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('experiments/results_df.csv') # TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6869a3fcd5fc665c02cfd6671afb83b30427e0c0f4ed20c6135df23c280a7ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
